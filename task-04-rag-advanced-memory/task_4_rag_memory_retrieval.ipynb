{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN49G9BKwASXygOozVGZWRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalks02/finwise-genai-capstone/blob/task-04-rag-advanced-memory/task_4_rag_memory_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNwMAj-95rD5"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain-google-genai faiss-cpu sentence-transformers pypdf langchain-community\n",
        "\n",
        "import os\n",
        "from pypdf import PdfReader   # Better PDF parser\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ============= 1. Set API Key Manually =============\n",
        "GOOGLE_API_KEY = \"AIzaSyBxOCf3v8px5yoqjPEKQmRWrl-9EShPo9c\"   # üîë replace with your actual Gemini API key\n",
        "\n",
        "# Configure Google GenAI SDK\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# ============= 2. Load PDF and split text =============\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        reader = PdfReader(pdf)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def split_text(text, chunk_size=500, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# ============= 3. Build FAISS Vector DB =============\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def create_vector_store(chunks):\n",
        "    return FAISS.from_texts(chunks, embedding=embedder)\n",
        "\n",
        "# ============= 4. Setup LLM (Gemini Pro with manual key) =============\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3,\n",
        "    google_api_key=GOOGLE_API_KEY   # ‚úÖ this avoids RefreshError\n",
        ")\n",
        "\n",
        "# ============= 5. Add MultiQueryRetriever =============\n",
        "def build_retriever(vectorstore):\n",
        "    return MultiQueryRetriever.from_llm(\n",
        "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "# ============= 6. Add Memory =============\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key='answer' # Add this line to explicitly set the output key\n",
        ")\n",
        "\n",
        "# ============= 7. Build Conversational Chain =============\n",
        "def build_conversational_chain(vectorstore):\n",
        "    retriever = build_retriever(vectorstore)\n",
        "    return ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "# ============= 8. Run Q&A Session =============\n",
        "pdf_docs = [\"kyc_compliance_report.pdf\"]   # replace with your PDF file\n",
        "raw_text = get_pdf_text(pdf_docs)\n",
        "chunks = split_text(raw_text)\n",
        "\n",
        "if not chunks:\n",
        "    raise ValueError(\"‚ö†Ô∏è No extractable text found in the PDF. If it's scanned, use OCR.\")\n",
        "\n",
        "vectorstore = create_vector_store(chunks)\n",
        "qa_chain = build_conversational_chain(vectorstore)\n",
        "\n",
        "print(\"‚úÖ PDF ready! Start asking questions...\\n\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"Your Question (or 'exit'): \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    result = qa_chain({\"question\": query})\n",
        "    print(\"\\nAnswer:\", result[\"answer\"])\n",
        "    print(\"-\" * 60)"
      ]
    }
  ]
}